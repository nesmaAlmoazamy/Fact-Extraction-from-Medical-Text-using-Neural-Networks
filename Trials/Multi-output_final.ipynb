{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = pd.read_csv(\"egcut_epi_nesma_extracted_features_example.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.columns = [\"text_ID\",\"start\",\"end\",\"object\",\"value\",\"unit\",\"min\",\"max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"egcut_epi_nesma_texts_example.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347474</td>\n",
       "      <td>\\nRR 132/84  \\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46809</td>\n",
       "      <td>RR 130/80 mmHg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45389</td>\n",
       "      <td>Kergelt liigkaaluline. Nahk, limaskestad taval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47482</td>\n",
       "      <td>RR142/89mmHg, HR 76 x min, p167cm, k89kg.  EKG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53393</td>\n",
       "      <td>RR143/87mmHg, HR60 x min, p180cm, k93kg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_ID                                               text\n",
       "0   347474                                  \\nRR 132/84  \\n\\n\n",
       "1    46809                                    RR 130/80 mmHg.\n",
       "2    45389  Kergelt liigkaaluline. Nahk, limaskestad taval...\n",
       "3    47482  RR142/89mmHg, HR 76 x min, p167cm, k89kg.  EKG...\n",
       "4    53393            RR143/87mmHg, HR60 x min, p180cm, k93kg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.columns = [\"text_ID\",\"text\"]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.text_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.drop_duplicates(subset='text_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.merge(extract, text, how=\"left\", on='text_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DS.text\n",
    "Y = DS.object\n",
    "Z = DS.value\n",
    "X = X[:5000]\n",
    "Y = Y[:5000]\n",
    "Z = Z[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import GRU,Dropout,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest , Ztrain, Ztest = train_test_split(X, Y, Z, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain, Xval, Ytrain, Yval , Ztrain, Zval= train_test_split(Xtrain, Ytrain, Ztrain, test_size=0.25, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 3\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y) #3 classes in Object column\n",
    "\n",
    "\n",
    "Ytrain = encoder.transform(Ytrain)\n",
    "Ytest = encoder.transform(Ytest)\n",
    "Yval = encoder.transform(Yval)\n",
    "\n",
    "Ytrain = np_utils.to_categorical(Ytrain)\n",
    "Ytest = np_utils.to_categorical(Ytest)\n",
    "Yval = np_utils.to_categorical(Yval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_sequences(Xtrain)\n",
    "Xtest = tokenizer.texts_to_sequences(Xtest)\n",
    "Xval = tokenizer.texts_to_sequences(Xval)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "Xtrain = pad_sequences(Xtrain, padding='post', maxlen=maxlen)\n",
    "Xtest = pad_sequences(Xtest, padding='post', maxlen=maxlen)\n",
    "Xval = pad_sequences(Xval, padding='post', maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 256)     9718272     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          525312      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 64)           16448       lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 32)           2080        dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 32)           1056        dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 3)            99          dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dense_21[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,263,300\n",
      "Trainable params: 10,263,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(keras.layers.LSTM(100, input_shape=(Xtrain.shape[0], Xtrain.shape[1]), return_sequences='true'))\n",
    "# # model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(250,return_sequences=True))\n",
    "# # model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.Dense(64))\n",
    "# model.add(keras.layers.Activation(\"linear\"))\n",
    "# model.add(keras.layers.Dense(2, activation='linear'))\n",
    "# model.add(keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "input_text = keras.layers.Input(shape=(100,))\n",
    "emb = Embedding(input_dim=37962, output_dim=256, input_length=100)(input_text)\n",
    "\n",
    "x = keras.layers.LSTM(256)(emb)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "# x = keras.layers.Flatten()(x)\n",
    "output_object = keras.layers.Dense(3, activation='linear')(x)\n",
    "output_value = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "\n",
    "model = keras.models.Model(inputs=input_text,\n",
    "                                outputs=[output_object,output_value])\n",
    "\n",
    "model.compile(optimizer='adam',loss=[ \"categorical_crossentropy\",\"mean_squared_error\"],metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 10457817.5339 - dense_22_loss: 1.0908 - dense_23_loss: 10457816.0000 - dense_22_accuracy: 0.5630 - dense_23_accuracy: 0.0010 - val_loss: 207296.1875 - val_dense_22_loss: 1.0937 - val_dense_23_loss: 207295.0938 - val_dense_22_accuracy: 0.5520 - val_dense_23_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 10456130.1055 - dense_22_loss: 1.0982 - dense_23_loss: 10456129.0000 - dense_22_accuracy: 0.5630 - dense_23_accuracy: 0.0020 - val_loss: 206015.5156 - val_dense_22_loss: 1.1051 - val_dense_23_loss: 206014.4062 - val_dense_22_accuracy: 0.5520 - val_dense_23_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10453387.5869 - dense_22_loss: 1.1066 - dense_23_loss: 10453386.0000 - dense_22_accuracy: 0.5630 - dense_23_accuracy: 0.0027 - val_loss: 204569.8594 - val_dense_22_loss: 1.1138 - val_dense_23_loss: 204568.7500 - val_dense_22_accuracy: 0.1400 - val_dense_23_accuracy: 0.0010\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10450563.9691 - dense_22_loss: 1.1182 - dense_23_loss: 10450563.0000 - dense_22_accuracy: 0.1273 - dense_23_accuracy: 0.0037 - val_loss: 203016.8438 - val_dense_22_loss: 1.1233 - val_dense_23_loss: 203015.7188 - val_dense_22_accuracy: 0.1400 - val_dense_23_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10448271.1247 - dense_22_loss: 1.1290 - dense_23_loss: 10448271.0000 - dense_22_accuracy: 0.1273 - dense_23_accuracy: 0.0037 - val_loss: 201289.7500 - val_dense_22_loss: 1.1332 - val_dense_23_loss: 201288.6094 - val_dense_22_accuracy: 0.1400 - val_dense_23_accuracy: 0.0030\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10444456.9893 - dense_22_loss: 1.1319 - dense_23_loss: 10444456.0000 - dense_22_accuracy: 0.1273 - dense_23_accuracy: 0.0040 - val_loss: 199699.6875 - val_dense_22_loss: 1.1225 - val_dense_23_loss: 199698.5625 - val_dense_22_accuracy: 0.1400 - val_dense_23_accuracy: 0.0040\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10440896.6515 - dense_22_loss: 1.1197 - dense_23_loss: 10440896.0000 - dense_22_accuracy: 0.1273 - dense_23_accuracy: 0.0037 - val_loss: 198267.2812 - val_dense_22_loss: 1.1057 - val_dense_23_loss: 198266.1719 - val_dense_22_accuracy: 0.1400 - val_dense_23_accuracy: 0.0060\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 20s 7ms/step - loss: 10437667.9905 - dense_22_loss: 1.0982 - dense_23_loss: 10437667.0000 - dense_22_accuracy: 0.4130 - dense_23_accuracy: 0.0060 - val_loss: 197140.6562 - val_dense_22_loss: 1.0820 - val_dense_23_loss: 197139.5781 - val_dense_22_accuracy: 0.5520 - val_dense_23_accuracy: 0.0250\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 10433522.1927 - dense_22_loss: 1.0729 - dense_23_loss: 10433521.0000 - dense_22_accuracy: 0.5630 - dense_23_accuracy: 0.0093 - val_loss: 196606.2500 - val_dense_22_loss: 1.0594 - val_dense_23_loss: 196605.1875 - val_dense_22_accuracy: 0.5520 - val_dense_23_accuracy: 0.0060\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 19s 6ms/step - loss: 10430258.1086 - dense_22_loss: 1.0494 - dense_23_loss: 10430257.0000 - dense_22_accuracy: 0.5630 - dense_23_accuracy: 0.0043 - val_loss: 196818.7812 - val_dense_22_loss: 1.0349 - val_dense_23_loss: 196817.7500 - val_dense_22_accuracy: 0.5520 - val_dense_23_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "tryx = Xtrain.reshape((3000,100))\n",
    "history = model.fit(tryx, y = [Ytrain,Ztrain],\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(Xval, [Yval,  Zval]),batch_size=1000)\n",
    "#,batch_size=5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 17s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10428394.656989584,\n",
       " 1.0307753086090088,\n",
       " 10400667.0,\n",
       " 0.5630000233650208,\n",
       " 0.0026666666381061077]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f= open(\"report.csv\",\"a\")\n",
    "# loss1, accuracy1,loss2, accuracy2 = \n",
    "model.evaluate(Xtrain, [Ytrain,Ztrain], verbose=True)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# f.write(\",{:.4f}\".format(accuracy))\n",
    "\n",
    "# loss, accuracy = model.evaluate(Xval, Yval, verbose=True)\n",
    "\n",
    "# print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
    "# f.write(\",{:.4f} \\n\".format(accuracy))\n",
    "# f.close()\n",
    "# plot_history(history)\n",
    "# Ypred = model.predict(Xtest)\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# matrix = confusion_matrix(Ytest.argmax(axis=1), Ypred.argmax(axis=1))\n",
    "# classification_Report = classification_report(Ytest.argmax(axis=1), Ypred.argmax(axis=1), output_dict=True)\n",
    "# df = pd.DataFrame(classification_Report).transpose()\n",
    "# df.to_csv(\"ClassificationReport.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1,pred2 = model.predict(Xval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15979794.698929688,\n",
       " 1.0298240184783936,\n",
       " 15605314.0,\n",
       " 0.5659999847412109,\n",
       " 0.004000000189989805]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, [Ytest,Ztest], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[14.758798 , 21.048134 , 15.976868 ],\n       [14.7624035, 21.054    , 15.981289 ],\n       [14.765573 , 21.057665 , 15.984047 ],\n       ...,\n       [14.764987 , 21.057224 , 15.983606 ],\n       ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-929e2293d189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mYval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[14.758798 , 21.048134 , 15.976868 ],\n       [14.7624035, 21.054    , 15.981289 ],\n       [14.765573 , 21.057665 , 15.984047 ],\n       ...,\n       [14.764987 , 21.057224 , 15.983606 ],\n       ..."
     ]
    }
   ],
   "source": [
    "model.evaluate([pred1,pred2],[Yval,Zval], verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
