{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = pd.read_csv(\"egcut_epi_nesma_extracted_features_example.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract.columns = [\"text_ID\",\"start\",\"end\",\"object\",\"value\",\"unit\",\"min\",\"max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"egcut_epi_nesma_texts_example.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347474</td>\n",
       "      <td>\\nRR 132/84  \\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46809</td>\n",
       "      <td>RR 130/80 mmHg.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45389</td>\n",
       "      <td>Kergelt liigkaaluline. Nahk, limaskestad taval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47482</td>\n",
       "      <td>RR142/89mmHg, HR 76 x min, p167cm, k89kg.  EKG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53393</td>\n",
       "      <td>RR143/87mmHg, HR60 x min, p180cm, k93kg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_ID                                               text\n",
       "0   347474                                  \\nRR 132/84  \\n\\n\n",
       "1    46809                                    RR 130/80 mmHg.\n",
       "2    45389  Kergelt liigkaaluline. Nahk, limaskestad taval...\n",
       "3    47482  RR142/89mmHg, HR 76 x min, p167cm, k89kg.  EKG...\n",
       "4    53393            RR143/87mmHg, HR60 x min, p180cm, k93kg"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.columns = [\"text_ID\",\"text\"]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.text_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.drop_duplicates(subset='text_ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = pd.merge(extract, text, how=\"left\", on='text_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DS.text\n",
    "Y = DS.object\n",
    "Z = DS.value\n",
    "Span = DS[[\"start\",\"end\"]]\n",
    "X = X[:5000]\n",
    "Span = Span[:5000] \n",
    "Y = Y[:5000]\n",
    "Z = Z[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import GRU,Dropout,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_history(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_acc']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "#     x = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(x, acc, 'b', label='Training acc')\n",
    "#     plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(x, loss, 'b', label='Training loss')\n",
    "#     plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest , Ztrain, Ztest ,Strain,Stext = train_test_split(X, Y, Z,Span, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain, Xval, Ytrain, Yval , Ztrain, Zval , Strain, Sval= train_test_split(Xtrain, Ytrain, Ztrain, Strain, test_size=0.25, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = 3\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y) #3 classes in Object column\n",
    "\n",
    "\n",
    "Ytrain = encoder.transform(Ytrain)\n",
    "Ytest = encoder.transform(Ytest)\n",
    "Yval = encoder.transform(Yval)\n",
    "\n",
    "Ytrain = np_utils.to_categorical(Ytrain)\n",
    "Ytest = np_utils.to_categorical(Ytest)\n",
    "Yval = np_utils.to_categorical(Yval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "\n",
    "Xtrain = tokenizer.texts_to_sequences(Xtrain)\n",
    "Xtest = tokenizer.texts_to_sequences(Xtest)\n",
    "Xval = tokenizer.texts_to_sequences(Xval)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "Xtrain = pad_sequences(Xtrain, padding='post', maxlen=maxlen)\n",
    "Xtest = pad_sequences(Xtest, padding='post', maxlen=maxlen)\n",
    "Xval = pad_sequences(Xval, padding='post', maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Strain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_text (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 100, 256)     9718272     input_text[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 256)          525312      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "span (InputLayer)               (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 258)          0           lstm_8[0][0]                     \n",
      "                                                                 span[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           16576       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           2080        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 32)           1056        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 3)            99          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            33          dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,263,428\n",
      "Trainable params: 10,263,428\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(keras.layers.LSTM(100, input_shape=(Xtrain.shape[0], Xtrain.shape[1]), return_sequences='true'))\n",
    "# # model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.LSTM(250,return_sequences=True))\n",
    "# # model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.Dense(64))\n",
    "# model.add(keras.layers.Activation(\"linear\"))\n",
    "# model.add(keras.layers.Dense(2, activation='linear'))\n",
    "# model.add(keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "input_text = keras.layers.Input(shape=(100,),dtype='int32',name=\"input_text\")\n",
    "emb = Embedding(input_dim=37962, output_dim=256, input_length=100)(input_text)\n",
    "lstm_out = keras.layers.LSTM(256)(emb)\n",
    "\n",
    "span = keras.layers.Input(shape=(2, ),name=\"span\")\n",
    "merged = keras.layers.concatenate([lstm_out,span])\n",
    "\n",
    "x = keras.layers.Dense(64, activation='relu')(merged)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "# x = keras.layers.Flatten()(x)\n",
    "output_object = keras.layers.Dense(3, activation='linear')(x)\n",
    "output_value = keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "\n",
    "model = keras.models.Model(inputs=[input_text,span],\n",
    "                                outputs=[output_object,output_value])\n",
    "\n",
    "model.compile(optimizer='adam',loss=[ \"categorical_crossentropy\",\"mean_squared_error\"],metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kasutaja\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 10465262.1602 - dense_11_loss: 8.9807 - dense_12_loss: 10465253.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0013 - val_loss: 210758.4062 - val_dense_11_loss: 8.1063 - val_dense_12_loss: 210750.2969 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0010\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 10463839.7135 - dense_11_loss: 7.8882 - dense_12_loss: 10463832.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 3.3333e-04 - val_loss: 209742.9219 - val_dense_11_loss: 6.6509 - val_dense_12_loss: 209736.2656 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0020\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 10462053.0339 - dense_11_loss: 6.1985 - dense_12_loss: 10462047.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0027 - val_loss: 209637.6406 - val_dense_11_loss: 5.6213 - val_dense_12_loss: 209632.0156 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0060\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 10461590.2135 - dense_11_loss: 5.4663 - dense_12_loss: 10461585.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0030 - val_loss: 209358.9375 - val_dense_11_loss: 5.0673 - val_dense_12_loss: 209353.8750 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0010\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 10460548.7604 - dense_11_loss: 5.4885 - dense_12_loss: 10460543.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0023 - val_loss: 208417.3281 - val_dense_11_loss: 5.5347 - val_dense_12_loss: 208411.7969 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0010\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 14s 5ms/step - loss: 10459130.6423 - dense_11_loss: 5.5876 - dense_12_loss: 10459125.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0020 - val_loss: 207435.5312 - val_dense_11_loss: 5.5833 - val_dense_12_loss: 207429.9531 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0040\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 10457728.6924 - dense_11_loss: 5.6245 - dense_12_loss: 10457723.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0053 - val_loss: 206548.8750 - val_dense_11_loss: 6.0393 - val_dense_12_loss: 206542.8281 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0030\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 10456268.4583 - dense_11_loss: 5.9472 - dense_12_loss: 10456263.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0020 - val_loss: 205573.4688 - val_dense_11_loss: 6.2058 - val_dense_12_loss: 205567.2656 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0030\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 10454590.1582 - dense_11_loss: 6.2555 - dense_12_loss: 10454584.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0027 - val_loss: 204642.8438 - val_dense_11_loss: 6.6892 - val_dense_12_loss: 204636.1562 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0060\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 15s 5ms/step - loss: 10452793.9115 - dense_11_loss: 6.6125 - dense_12_loss: 10452787.0000 - dense_11_accuracy: 0.3097 - dense_12_accuracy: 0.0040 - val_loss: 203642.6250 - val_dense_11_loss: 6.9169 - val_dense_12_loss: 203635.7031 - val_dense_11_accuracy: 0.3080 - val_dense_12_accuracy: 0.0030\n"
     ]
    }
   ],
   "source": [
    "tryx = Xtrain.reshape((3000,100))\n",
    "history = model.fit([tryx,Strain], y = [Ytrain,Ztrain],\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=([Xval,Sval], [Yval,  Zval]),batch_size=1000)\n",
    "#,batch_size=5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 17s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10428394.656989584,\n",
       " 1.0307753086090088,\n",
       " 10400667.0,\n",
       " 0.5630000233650208,\n",
       " 0.0026666666381061077]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f= open(\"report.csv\",\"a\")\n",
    "# loss1, accuracy1,loss2, accuracy2 = \n",
    "model.evaluate(Xtrain, [Ytrain,Ztrain], verbose=True)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# f.write(\",{:.4f}\".format(accuracy))\n",
    "\n",
    "# loss, accuracy = model.evaluate(Xval, Yval, verbose=True)\n",
    "\n",
    "# print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n",
    "# f.write(\",{:.4f} \\n\".format(accuracy))\n",
    "# f.close()\n",
    "# plot_history(history)\n",
    "# Ypred = model.predict(Xtest)\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# matrix = confusion_matrix(Ytest.argmax(axis=1), Ypred.argmax(axis=1))\n",
    "# classification_Report = classification_report(Ytest.argmax(axis=1), Ypred.argmax(axis=1), output_dict=True)\n",
    "# df = pd.DataFrame(classification_Report).transpose()\n",
    "# df.to_csv(\"ClassificationReport.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1,pred2 = model.predict(Xval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 6s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15979794.698929688,\n",
       " 1.0298240184783936,\n",
       " 15605314.0,\n",
       " 0.5659999847412109,\n",
       " 0.004000000189989805]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Xtest, [Ytest,Ztest], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[14.758798 , 21.048134 , 15.976868 ],\n       [14.7624035, 21.054    , 15.981289 ],\n       [14.765573 , 21.057665 , 15.984047 ],\n       ...,\n       [14.764987 , 21.057224 , 15.983606 ],\n       ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-929e2293d189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mYval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\myEnv\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[14.758798 , 21.048134 , 15.976868 ],\n       [14.7624035, 21.054    , 15.981289 ],\n       [14.765573 , 21.057665 , 15.984047 ],\n       ...,\n       [14.764987 , 21.057224 , 15.983606 ],\n       ..."
     ]
    }
   ],
   "source": [
    "model.evaluate([pred1,pred2],[Yval,Zval], verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
